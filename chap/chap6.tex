% Copyright (c) 2014,2016,2018 Casper Ti. Vector
% Public domain.

\chapter{性能测试结果及分析}
%\pkuthssffaq % 中文测试文字。
\section{实验配置}
这一章测试了我们实现的 exFAT 文件系统的性能，并与 Linux 中的对应实现进行了比较。
由于目前 Asterinas 系统还处于开发阶段，还不能实机运行，只能使用 Qemu\parencite{qemu} 进行测试。
故为了确保比较的公平，Linux 的实验结果也是通过 Qemu 取得。
另外，Linux 内核出于某些原因没有预设支持读写 exFAT 文件系统的驱动程序，下面的实验结果是通过 FUSE 套件\parencite{fuse}取得，
可能对性能有一定影响，所以实验结果仅供参考。
我们进行的测试主要目的是测试文件 IO 性能，如果没有特殊说明，测试的默认情景是文件已经存在并且初始化完成，
页缓存在测试开始时为空，在这种情况下通过 FIO\parencite{fio} 触发文件读写并得到测试结果。
表\ref{tab:exp_setup}总结了本节实验的一些通用配置。

\begin{table}[h]
    \centering
    \begin{tabularx}{\textwidth}{|Y|Y|}
    \hline
    项目 & 描述 \\
    \hline
    存储设备大小 & 4G \\
    \hline
    系统内存大小 & 4G \\
    \hline
    测试文件大小 & 1G \\
    \hline
    单次请求大小 & 4K \\
    \hline
    FIO 测试读写引擎 & sync \\
    \hline
    Linux exFAT套件版本 & FUSE exfat 1.3.0+git20220115 \\
    \hline
    \end{tabularx}
    \caption{实验配置}
    \label{tab:exp_setup}
\end{table}

\section{exFAT文件系统性能测试}\label{sec:exp_file_io}

 \begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{new_cmp.eps}
    \caption{文件 IO 带宽对比}
    \label{fig:new_cmp}
\end{figure}

我们测试了在不同读写模式下，初始页缓存为空的状态下，以单次请求大小 4k 为粒度，对测试文件进行累计 1G 读写的平均带宽。
图\ref{fig:new_cmp}（a）展示了使用页缓存情况下的 IO 带宽，分别比较了使用数据预取的 Asterinas 系统、不使用数据预取的 Asterinas 系统以及 Linux 系统
在顺序读、顺序写、随机读、随机写模式下的 IO 带宽。
图\ref{fig:new_cmp}（b）则展示了不使用页缓存的情况下的 IO 带宽， 仍然是比较了Asterinas 系统（这时是否使用数据预取没有区别）
和 Linux 系统在四种 IO 模式下的带宽。

从图\ref{fig:new_cmp}（a）中可以看出，在使用页缓存的情况下，数据预取机制的引入明显提高了顺序访问模式下的 IO 带宽，
而随机访问模式的 IO 带宽没有受到明显影响，整体的 IO 表现符合预期。
此外，通过和 Linux 的对比可以看出，我们的实现在顺序读写模式下慢于 Linux，在随机读写模式下快于 Linux。
考虑到 Linux 的实验结果是通过 FUSE 套件取得，会对性能有一定影响，故这个比较仅供参考。
在不使用页缓存的情况下，所有的 IO 均直接与底层块设备进行交互，测得的性能与对应模式在使用页缓存但不做数据预取时接近。
图\ref{fig:new_cmp}（b）中和 Linux 对应性能的对比可以看出，Asterinas 系统直接读写块设备的速度普遍慢于 Linux 系统。
这其中的原因可能是 Asterinas 的块设备层的实现相对简单，进行测试时 Asterinas 的底层块设备使用的是同步的实现，
页缓存向块设备提交的数据预取请求并不会在后台自动完成，不能很好的掩盖掉块设备的访问开销。


\section{数据预取性能测试}

为了进一步观察我们为页缓存实现数据预取所带来的性能提升，我们又测试了在两种不同的页缓存配置下，使用各种不同的 IO 模式，
对同一文件进行的连续十次相同 IO 的性能变化。
每次 IO 定义为使用相应的读写模式，以单次请求大小 4k 为粒度，对测试文件进行累计 1G 读写。
在第一次 IO 开始前，页缓存是空的，此后的每一次测试都是紧接着前一次测试进行，不清空页缓存。

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{cmp_combined.eps}
    \caption{使用数据预取与否对性能的影响}
    \label{fig:cmp_ra}
\end{figure}

图\ref{fig:cmp_ra}中展示了我们的测试结果，每张图对应一种 IO 模式，图中的两根曲线分别代表使用数据预取和不使用数据预取的 Asterinas 系统。
从图中可以看出，我们的数据预取的引入主要提升的是顺序 IO 模式的性能表现，对于随机 IO 模式，是否使用数据预取对性能影响不大。
对于顺序 IO 模式，使用数据预取能更快的达到性能峰值，此外，还可以减少性能的波动，使性能维持在峰值附近。
这个结果与\ref{sec:exp_file_io}节的实验结果相符，正是因为 IO 性能能更快的达到峰值，才使得增加数据预取后的总的 IO 性能得到提升。
但从图中还可以看出，我们的数据预取仍有提升的空间，在最理想的情况下顺序 IO 的性能可以在首次调用时便达到峰值，
在这种情况下，上层应用可以完全感知不到块设备层的存在，对于上层应用来说，文件仿佛始终位于内存之中。
这可能的原因仍然是是测试时 Asterinas 系统的块设备层使用的是同步的实现，对此继续改进的方向可能是为块设备层提供异步 IO 支持。
% vim:ts=4:sw=4
