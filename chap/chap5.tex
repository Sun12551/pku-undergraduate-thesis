% Copyright (c) 2014,2016,2018 Casper Ti. Vector
% Public domain.

\chapter{实验结果及分析}
%\pkuthssffaq % 中文测试文字。
\section{实验配置}
这一章测试了我们实现的exFAT文件系统的性能，并与 Linux 中的对应实现进行了比较。
由于目前 Asterinas 系统还处于开发阶段，还不能实机运行，只能使用 Qemu 进行测试。
故为了确保比较的公平，Linux 的实验结果也是通过 Qemu 取得。
另外，Linux 内核出于某些原因没有预设支持读写 exFAT 文件系统的驱动程序，下面的实验结果是通过 FUSE 套件取得，
可能对性能有一定影响，所以实验结果仅供参考。
我们进行的测试主要目的是测试文件 IO 性能，如果没有特殊说明，测试的默认情景是文件已经存在并且初始化完成，
页缓存在测试开始时为空，在这种情况下通过 FIO 触发文件读写并得到测试结果。
表\ref{tab:exp_setup}总结了本节实验的一些通用配置。

\begin{table}[h]
    \centering
    \begin{tabularx}{\textwidth}{|Y|Y|}
    \hline
    项目 & 描述 \\
    \hline
    存储设备大小 & 4G \\
    \hline
    系统内存大小 & 4G \\
    \hline
    测试文件大小 & 1G \\
    \hline
    单次请求大小 & 4K \\
    \hline
    FIO 测试读写引擎 & sync \\
    \hline
    Linux exFAT套件版本 & FUSE exfat 1.3.0+git20220115 \\
    \hline
    \end{tabularx}
    \caption{实验配置}
    \label{tab:exp_setup}
\end{table}

\section{exFAT文件系统性能测试}

我们比较了在不同读写模式下，初始页缓存为空的状态下，以单次请求大小为粒度，对测试文件进行连续 60s IO 操作的平均带宽。
图xx展示了使用页缓存情况下的读写带宽，分别比较了使用数据预取的 Asterinas 系统、不使用数据预取的 Asterinas 系统以及 Linux 系统
在顺序读、顺序写、随机读、随机写模式下的 IO 带宽。
从图中可以看出，页缓存预取机制的引入提高了顺序访问模式下的 IO 带宽，而随机访问模式的 IO 带宽没有受到明显影响，整体的 IO 表现符合预期。
此外，通过和 Linux 的对比可以看出，我们的实现在顺序读模式下明显慢于 Linux，但其余 IO 模式都快于 Linux。
考虑到 Linux 的实验结果是通过 FUSE 套件取得，会对性能有一定影响，故这个比较仅供参考。

图xx展示了不使用页缓存的情况下 Asterinas 和 Linux 系统的读写带宽。
在不使用缓存的情况下，所有的 IO 均直接与底层块设备进行交互，自然会慢于使用页缓存的结果。
测试结果表明，Asterinas 系统的直接读写速度普遍慢于 Linux 系统，
这可能是由于测试时的 Asterinas 系统还处于研发初期，其中的块设备层只能支持同步 IO，而 Linux 系统的块设备层存在异步 IO。

\section{数据预取性能测试}

为了进一步观察我们为页缓存实现数据预取所带来的性能提升，我们又测试了在两种不同的页缓存配置下，使用各种不同的 IO 模式，
对同一文件进行的连续十次相同 IO 的性能变化。
每次 IO 定义为使用相应的读写模式，以单次请求大小为粒度，总计请求和测试文件大小相同的数据。
在第一次 IO 开始前，页缓存是空的，此后的每一次测试都是紧接着上一次测试进行，不清空页缓存，
图xx中展示了我们的测试结果，每张图对应一种 IO 模式，图中的两根曲线分别代表使用数据预取和不使用数据预取的 Asterinas 系统。
从图中可以看出，对于顺序 IO 模式，使用数据预取能更快的达到性能峰值；对于随机读写模式，是否使用数据预取影响不大。
这也符合我们的预期，因为我们只针对顺序 IO 模式进行数据预取。
从图中还可以看出，我们的数据预取仍有提升的空间，在最理想的情况下顺序 IO 的性能可以在首次调用时便达到峰值，
上层应用可以完全感知不到块设备层的存在。
这可能是因为测试时 Asterinas 系统的块设备层仍是同步的实现，之后继续改进的可能方向是为块设备层提供异步支持。
% vim:ts=4:sw=4
